{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f475c855-1380-4e96-890c-d8dc538c8743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V.Thanushan ITBIN-2211-0103-Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4df7b102-260a-4d9f-8f7f-65626548bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.image import load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ab5127-87d0-438a-b937-d557f8bd8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_DIR = 'images/train'\n",
    "TEST_DIR = 'images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63292b3b-3e41-4508-bfa4-3f7910ddf195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir,label)):\n",
    "            image_paths.append(os.path.join(dir,label,imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return image_paths,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66cd49cb-3ff0-478c-88b8-0e60ae38e7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = r\"C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Emotion Recognition\\archive\\images\\train\"\n",
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = createdataframe(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2015598d-ce4f-4097-b011-01f8eba1b08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   image     label\n",
      "0      C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...     angry\n",
      "1      C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...     angry\n",
      "2      C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...     angry\n",
      "3      C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...     angry\n",
      "4      C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...     angry\n",
      "...                                                  ...       ...\n",
      "28816  C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...  surprise\n",
      "28817  C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...  surprise\n",
      "28818  C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...  surprise\n",
      "28819  C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...  surprise\n",
      "28820  C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...  surprise\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b493460-00be-4288-a3ca-60d1a645596a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "TEST_DIR = r\"C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Emotion Recognition\\archive\\images\\train\"\n",
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = createdataframe(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64efd4cf-2311-45a0-a7f9-483bc3be3898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   image     label\n",
      "0      C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...     angry\n",
      "1      C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...     angry\n",
      "2      C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...     angry\n",
      "3      C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...     angry\n",
      "4      C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...     angry\n",
      "...                                                  ...       ...\n",
      "28816  C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...  surprise\n",
      "28817  C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...  surprise\n",
      "28818  C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...  surprise\n",
      "28819  C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...  surprise\n",
      "28820  C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...  surprise\n",
      "\n",
      "[28821 rows x 2 columns]\n",
      "0        C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...\n",
      "1        C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...\n",
      "2        C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...\n",
      "3        C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...\n",
      "4        C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...\n",
      "                               ...                        \n",
      "28816    C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...\n",
      "28817    C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...\n",
      "28818    C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...\n",
      "28819    C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...\n",
      "28820    C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Em...\n",
      "Name: image, Length: 28821, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf86d187-b2f3-4351-861e-eec877899707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae5cbb2e-77c7-49eb-b97b-ebf682274049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image,grayscale =  True )\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(len(features),48,48,1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31e76c-8f99-426b-881c-4c9ce74bfbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00957b68efa2407182ee1077968c76b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Emotion Recognition\\.venv\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "train_features = extract_features(train['image']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8be798-d4f2-4991-b1ef-26d355867e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = extract_features(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b2c70-42b3-4636-b5e7-376c9e53baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features/255.0\n",
    "x_test = test_features/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4512a1-91e0-4b92-a2dc-99774dbad021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b8349-066c-4322-a1af-38b28a48d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980907d9-91b6-4573-8d36-00514dbcb98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe31c16-0c1d-4d48-b0a0-da1fa314749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train,num_classes = 7)\n",
    "y_test = to_categorical(y_test,num_classes = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8d887-74d5-4674-99cd-b2886b6c830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#S.D.S.Waththegedara ITBIN-2211-0314-Model Building & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42177b1c-e4ab-465c-a95c-a3111de2c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "model = Sequential()\n",
    "# convolutional layers\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "# fully connected layers\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# output layer\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f346a2b-62bb-48e1-92ca-18e3b867249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd1379-deac-480f-87ac-1f16b78fb9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=x_train, y=y_train, batch_size=128, epochs=100, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51bb107-a13e-492b-9de0-f2a894d08ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    " #T .B.P.p.S.de silva ITBIN-2211-0167-Save, Load & Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a6ac6-03d5-4b25-bae6-00b55dc8dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.saving import save_model\n",
    "model_json = model.to_json()\n",
    "with open(\"emotiondetector.json\",'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotiondetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3c4bd-50f6-464f-b0eb-a15f22a24daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "with open(\"facialemotionmodel.json\", \"r\") as json_file:\n",
    "    model_json = json_file.read()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"facialemotionmodel.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e665f4-461d-4083-941a-0e67aeb656c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W.R.A.VidushanITBIN-2211-0167-Prediction & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c5c57-f99e-49fa-9294-4ab483088f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['angry','disgust','fear','happy','neutral','sad','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41986531-88a1-4561-8c06-697d6c32de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ef(image):\n",
    "    img = load_img(image,grayscale =  True )\n",
    "    feature = np.array(img)\n",
    "    feature = feature.reshape(1,48,48,1)\n",
    "    return feature/255.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da15762-ad0f-4a79-9465-f1e1d47dbdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14fa0c-ffe6-405f-8c84-934b60a001e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = r'C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Emotion Recognition\\archive\\images\\train\\sad\\7710.jpg'\n",
    "print(\"original image is of sad\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f3523-45c6-4a1a-b51d-9957b26c9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fbaf1e-34f2-4da7-8a4a-7762292116e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c4086-0e89-4293-9f29-ebb8a64bb5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = r'C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Emotion Recognition\\archive\\images\\train\\fear\\9.jpg'\n",
    "print(\"original image is of fear\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1ab659-d228-479c-90f1-5dd3a95e4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = r'C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Emotion Recognition\\archive\\images\\train\\angry\\0.jpg'\n",
    "print(\"original image is of angry\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5560ae-ee82-4abe-9f76-eabb64d84be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = r'C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Emotion Recognition\\archive\\images\\train\\disgust\\299.jpg'\n",
    "print(\"original image is of disgust\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822ebed5-b5ef-4df1-926b-b8ddc9b59512",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = r'C:\\Users\\ASUS\\Desktop\\Academy\\IT\\Pyhon\\Face Emotion Recognition\\archive\\images\\train\\surprise\\15.jpg'\n",
    "print(\"original image is of sad\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
